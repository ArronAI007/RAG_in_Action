{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3445c65c-9a2f-44bd-ac16-7c439918cb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件中的OpenAI API环境变量\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29a00e71-e9d1-4d19-9cea-7dc804832d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# 初始化向量数据库\n",
    "def initialize_faiss_vectorstore(file_paths):\n",
    "    # 1. 加载文档\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        loader = TextLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)\n",
    "\n",
    "    # 2. 分割文档\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 3. 初始化嵌入模型\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # 4. 创建并初始化FAISS向量数据库\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# 定义多重查询生成器\n",
    "def generate_queries(original_query):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a helpful assistant that generates multiple search queries based on a single input query.\\n\\nGenerate multiple search queries related to: {question}\\n\\nOutput (4 queries):\"\n",
    "    )\n",
    "    chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n",
    "    queries = chain.invoke({\"question\": original_query})[\"text\"].split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "# 定义倒数排名融合算法\n",
    "def reciprocal_rank_fusion(results, k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = str(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n",
    "# 定义完整的 RAG-Fusion 链\n",
    "def rag_fusion_chain(original_query):\n",
    "    # 生成多重查询\n",
    "    queries = generate_queries(original_query)\n",
    "    \n",
    "    # 对每个查询进行向量搜索\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        docs = vectorstore.similarity_search(query, k=4)\n",
    "        results.append(docs)\n",
    "    \n",
    "    # 使用 RRF 算法对文档进行重新排序\n",
    "    reranked_results = reciprocal_rank_fusion(results)\n",
    "    \n",
    "    # 提取重排序后的文档内容\n",
    "    top_docs = [doc[0].page_content for doc in reranked_results[:4]]\n",
    "    \n",
    "    # 生成最终答案\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Answer the following question based on this context:\\n{context}\\nQuestion: {question}\\n\"\n",
    "    )\n",
    "    chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n",
    "    answer = chain.invoke({\"context\": \"\\n\".join(top_docs), \"question\": original_query})\n",
    "    return answer\n",
    "\n",
    "# 示例问题\n",
    "original_query = \"示例查询内容\"\n",
    "file_paths = [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"]\n",
    "vectorstore = initialize_faiss_vectorstore(file_paths)\n",
    "result = rag_fusion_chain(original_query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f959200-9f50-4ed5-8d02-f9294838daf8",
   "metadata": {},
   "source": [
    "# 第二种实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675abe73-db08-4249-ba0f-77c0cf754e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# 1. 加载文档\n",
    "loader = TextLoader(\"path_to_your_document.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. 文档分块\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. 创建嵌入模型和向量存储\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 4. 定义查询生成函数\n",
    "def generate_queries(question):\n",
    "    # 这里可以使用任何方法生成多个查询，例如同义词替换、问题扩展等\n",
    "    queries = [\n",
    "        question,\n",
    "        f\"What is {question}?\",\n",
    "        f\"Explain {question} in detail.\",\n",
    "        f\"Can you provide more information about {question}?\"\n",
    "    ]\n",
    "    return queries\n",
    "\n",
    "# 5. 多轮检索\n",
    "def multi_retrieval(queries, vectorstore):\n",
    "    retrieved_docs = []\n",
    "    for query in queries:\n",
    "        docs = vectorstore.similarity_search(query, k=3)\n",
    "        retrieved_docs.extend(docs)\n",
    "    return retrieved_docs\n",
    "\n",
    "# 6. 结果融合（简单的去重）\n",
    "def fuse_results(retrieved_docs):\n",
    "    unique_docs = list({doc.page_content: doc for doc in retrieved_docs}.values())\n",
    "    return unique_docs\n",
    "\n",
    "# 7. 生成答案\n",
    "def generate_answer(question, docs):\n",
    "    llm = OpenAI(temperature=0)\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "    answer = chain.run(input_documents=docs, question=question)\n",
    "    return answer\n",
    "\n",
    "# 8. RAG-Fusion 主流程\n",
    "def rag_fusion(question):\n",
    "    queries = generate_queries(question)\n",
    "    retrieved_docs = multi_retrieval(queries, vectorstore)\n",
    "    fused_docs = fuse_results(retrieved_docs)\n",
    "    answer = generate_answer(question, fused_docs)\n",
    "    return answer\n",
    "\n",
    "# 9. 测试\n",
    "question = \"What is the capital of France?\"\n",
    "answer = rag_fusion(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_book",
   "language": "python",
   "name": "rag_book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
