{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7f9cc4-483a-4115-95ab-2794245841cf",
   "metadata": {},
   "source": [
    "# 一、基于文本长度的分块\n",
    "## 1.1 固定大小分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9111273f-4698-47fc-b0cb-d838766c6df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是一段很长的文本，', '用于测试分块功能。希', '望能够正确地将它分成', '多个块。']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"这是一段很长的文本，用于测试分块功能。希望能够正确地将它分成多个块。\"\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4b7d2-5b70-4d10-9f2c-b981a49b5427",
   "metadata": {},
   "source": [
    "## 加入chunk_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5532a192-24e5-4bfc-a68d-b722b116e643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是一段很长的文本，', '长的文本，用于测试分', '用于测试分块功能。希', '块功能。希望能够正确', '望能够正确地将它分成', '地将它分成多个块。']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"这是一段很长的文本，用于测试分块功能。希望能够正确地将它分成多个块。\"\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d42954-5575-4810-829c-180bf25d7c78",
   "metadata": {},
   "source": [
    "## 加入separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0785212d-52e9-4f93-b1d9-24f966c8934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 18, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是一段很长的文本，用于测试分块功能', '希望能够正确地将它分成多个块']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"这是一段很长的文本，用于测试分块功能。希望能够正确地将它分成多个块。\"\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"。\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588e345-0591-4deb-8fcb-529a1b9118b8",
   "metadata": {},
   "source": [
    "划分的块有可能会大于chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33103854-95e5-474c-b852-5d4f93bc55e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是一段很长的文本，', '用于测试分块功能。希', '望能够正确地将它分成', '多个块。']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"这是一段很长的文本，用于测试分块功能。希望能够正确地将它分成多个块。\"\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3f9d5-711c-47ef-84f3-c0ecf6b730b3",
   "metadata": {},
   "source": [
    "使用RecursiveCharacterTextSplitter可以保证输出块大小小于等于chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d1a744-e65b-432d-8299-535b242eb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是一个句子。这是另一个句子', '。这还是一个句子。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hezhidong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "text = \"这是一个句子。这是另一个句子。这还是一个句子。\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\", \"。\"]\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ba9c678-0ba0-4ce4-ab47-2013cc0349b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 18, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='这是一段很长的文本，用于测试分块功能'\n",
      "page_content='希望能够正确地将它分成多个块'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "test_text = \"\"\"这是一段很长的文本，用于测试分块功能。希望能够正确地将它分成多个块。\"\"\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"。\",\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 5,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "texts = text_splitter.create_documents([test_text])\n",
    "for text in texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6099d6f-04c4-4d2a-b606-8ce320bf8227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='这是一段很长的文本，用于测试分块功能。希望能够正确地将它分成多个块'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "test_text = \"\"\"这是一段很长的文本，用于测试分块功能。希望能够正确地将它分成多个块。\"\"\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"。\",\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 5,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "texts = text_splitter.create_documents([test_text])\n",
    "for text in texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ca95f-95fa-43ed-a2c4-bb38fc0c153b",
   "metadata": {},
   "source": [
    "## 1.2 可变长度分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c757f63-8cae-4144-8e7f-ab6871eb7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['摘要：这是一篇关于人工智能的论文摘要。', '实验过程：详细的实验过程描述，包括多个步骤和数据。', '结论：得出了一些重要结论。']\n"
     ]
    }
   ],
   "source": [
    "def variable_length_chunking(text, core_sections=[], core_chunk_size=300, other_chunk_size=800):\n",
    "    chunks = []\n",
    "    for i, section in enumerate(text.split(\"\\n\\n\")):\n",
    "        if i in core_sections:\n",
    "            for j in range(0, len(section), core_chunk_size):\n",
    "                chunks.append(section[j:j + core_chunk_size])\n",
    "        else:\n",
    "            for j in range(0, len(section), other_chunk_size):\n",
    "                chunks.append(section[j:j + other_chunk_size])\n",
    "    return chunks\n",
    "\n",
    "text = \"摘要：这是一篇关于人工智能的论文摘要。\\n\\n实验过程：详细的实验过程描述，包括多个步骤和数据。\\n\\n结论：得出了一些重要结论。\"\n",
    "chunks = variable_length_chunking(text, core_sections=[0, 2], core_chunk_size=300, other_chunk_size=800)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b27fc-1b1e-40fa-bc60-a08753fc98bc",
   "metadata": {},
   "source": [
    "# 二、基于语义的分块\n",
    "## 2.1 基于句子的分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fc678a-0f6d-4413-bda8-7bc5c5d4930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是一个句子。这是另一个句子。它们共同构成了一个段落。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hezhidong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_based_chunking(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < 500:\n",
    "            chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \" \"\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "text = \"这是一个句子。这是另一个句子。它们共同构成了一个段落。\"\n",
    "chunks = sentence_based_chunking(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3d513-7184-4162-aaa0-9b177538762b",
   "metadata": {},
   "source": [
    "## 2.2 基于段落的分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f1ab3b-1f72-4f61-8103-2426af808ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是第一段。', '这是第二段。', '这是第三段。']\n"
     ]
    }
   ],
   "source": [
    "def paragraph_based_chunking(text):\n",
    "    chunks = text.split(\"\\n\\n\")\n",
    "    return chunks\n",
    "\n",
    "text = \"这是第一段。\\n\\n这是第二段。\\n\\n这是第三段。\"\n",
    "chunks = paragraph_based_chunking(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ac762-d48b-4cfa-bdbb-e0d0e84d4d40",
   "metadata": {},
   "source": [
    "## 2.3 基于语义单元的分块（使用语言模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "805543d3-813a-4734-8f61-aee688b723d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_book/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK] [UNK] 一 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 的 文 本 ， [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 行 分 [UNK] [UNK] [UNK] 。']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def semantic_chunking(text, model_name=\"bert-base-uncased\", max_chunk_length=512):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_chunk_length, len(tokens))\n",
    "        input_ids = torch.tensor([tokens[start:end]])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            # 这里可以添加更复杂的语义相似性判断逻辑，暂时简化处理\n",
    "        chunks.append(tokenizer.decode(input_ids[0]))\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "text = \"这是一段语义较为复杂的文本，需要使用语言模型进行分块处理。\"\n",
    "chunks = semantic_chunking(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62655d-7357-4848-ae0b-14bd53e8c476",
   "metadata": {},
   "source": [
    "# 三、基于逻辑结构的分块\n",
    "## 3.1 基于标题的分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bbd4c93-b31d-4376-b6fb-a7fdf5e5265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['## 章节一\\n内容一。\\n', '## 章节二\\n内容二。']\n"
     ]
    }
   ],
   "source": [
    "def title_based_chunking(text, title_delimiter=\"##\"):\n",
    "    chunks = []\n",
    "    parts = text.split(title_delimiter)\n",
    "    for part in parts:\n",
    "        if part.strip():\n",
    "            chunks.append(title_delimiter + part)\n",
    "    return chunks\n",
    "\n",
    "text = \"## 章节一\\n内容一。\\n## 章节二\\n内容二。\"\n",
    "chunks = title_based_chunking(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1038ae2-dd32-470e-8ad6-6bec498c57ff",
   "metadata": {},
   "source": [
    "## 3.2 基于标记的分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "457d83ef-2dd9-4203-8768-1adb3fcedc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<item>内容一</item>', '<item>内容二</item>']\n"
     ]
    }
   ],
   "source": [
    "def tag_based_chunking(xml_text, tag=\"<item>\"):\n",
    "    import re\n",
    "    chunks = re.findall(f\"{tag}[^<]*</item>\", xml_text)\n",
    "    return chunks\n",
    "\n",
    "xml_text = \"<items><item>内容一</item><item>内容二</item></items>\"\n",
    "chunks = tag_based_chunking(xml_text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d2e4b-b7b4-4d0c-ac43-041330e33473",
   "metadata": {},
   "source": [
    "## 3.3 基于结构的分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae18339-cf02-48bf-948f-c208056fbbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a heading', 'This is a paragraph.', 'List item 1', 'List item 2']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 示例 HTML 文本\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>Page Title</title></head>\n",
    "<body>\n",
    "<h1>This is a heading</h1>\n",
    "<p>This is a paragraph.</p>\n",
    "<ul>\n",
    "<li>List item 1</li>\n",
    "<li>List item 2</li>\n",
    "</ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 使用 BeautifulSoup 解析 HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 定义基于结构的分块逻辑\n",
    "def split_by_structure(soup):\n",
    "    chunks = []\n",
    "    for tag in soup.find_all(True):  # 查找所有标签\n",
    "        if tag.name in ['p', 'h1', 'h2', 'h3', 'li']:  # 只关注某些类型的标签\n",
    "            chunks.append(tag.get_text())\n",
    "    return chunks\n",
    "\n",
    "# 执行基于结构的分块\n",
    "structure_chunks = split_by_structure(soup)\n",
    "print(structure_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30f322-cb5d-4de5-bb94-6e28ea776852",
   "metadata": {},
   "source": [
    "## 3.4  JSON数据分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "191df919-7342-4d4e-8ed6-9ae0ea02b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\\n    \"name\": \"张三\",\\n    \"age\": 30,\\n    \"address\": {\\n        \"street\": \"南京路\",\\n        \"city\": \"上海\",\\n        \"zipcode\": \"200000\"\\n    },\\n    \"hobbies\": [\\n        \"读书\",\\n        \"运动\",\\n        \"音乐\"\\n    ],', '\"work_experience\": [\\n        {\\n            \"company\": \"公司 A\",\\n            \"position\": \"软件工程师\",\\n            \"start_date\": \"2010-01-01\",\\n            \"end_date\": \"2015-12-31\"\\n        },\\n        {', '\"company\": \"公司 B\",\\n            \"position\": \"高级软件工程师\",\\n            \"start_date\": \"2016-01-01\",\\n            \"end_date\": \"2020-12-31\"\\n        }\\n    ]\\n}']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def json_to_text(json_data):\n",
    "    \"\"\"\n",
    "    将 JSON 数据转换为字符串格式\n",
    "    \"\"\"\n",
    "    import json\n",
    "    return json.dumps(json_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# 示例 JSON 数据\n",
    "json_data = {\n",
    "    \"name\": \"张三\",\n",
    "    \"age\": 30,\n",
    "    \"address\": {\n",
    "        \"street\": \"南京路\",\n",
    "        \"city\": \"上海\",\n",
    "        \"zipcode\": \"200000\"\n",
    "    },\n",
    "    \"hobbies\": [\"读书\", \"运动\", \"音乐\"],\n",
    "    \"work_experience\": [\n",
    "        {\n",
    "            \"company\": \"公司 A\",\n",
    "            \"position\": \"软件工程师\",\n",
    "            \"start_date\": \"2010-01-01\",\n",
    "            \"end_date\": \"2015-12-31\"\n",
    "        },\n",
    "        {\n",
    "            \"company\": \"公司 B\",\n",
    "            \"position\": \"高级软件工程师\",\n",
    "            \"start_date\": \"2016-01-01\",\n",
    "            \"end_date\": \"2020-12-31\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# 将 JSON 数据转换为文本\n",
    "text = json_to_text(json_data)\n",
    "\n",
    "\n",
    "# 使用 RecursiveCharacterTextSplitter 进行分块\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\", \" \", \"{\"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "\n",
    "# 对文本进行分块\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5dc092a-7aff-41d9-a6c0-2db080556547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '张三', 'age': 30, 'address': {'street': '南京路', 'city': '上海', 'zipcode': '200000'}, 'hobbies': ['读书', '运动', '音乐']}, {'work_experience': [{'company': '公司 A', 'position': '软件工程师', 'start_date': '2010-01-01', 'end_date': '2015-12-31'}, {'company': '公司 B', 'position': '高级软件工程师', 'start_date': '2016-01-01', 'end_date': '2020-12-31'}]}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "# 使用 RecursiveCharacterTextSplitter 进行分块\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "# 还可以设置min_chunk_size\n",
    "\n",
    "\n",
    "# 对文本进行分块\n",
    "chunks = json_splitter.split_json(json_data)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cea2089-29c0-4f27-8be2-15f6cf4aca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '张三', 'age': 30, 'address': {'street': '南京路', 'city': '上海', 'zipcode': '200000'}, 'hobbies': {'0': '读书', '1': '运动', '2': '音乐'}}, {'work_experience': {'0': {'company': '公司 A', 'position': '软件工程师', 'start_date': '2010-01-01', 'end_date': '2015-12-31'}}}, {'work_experience': {'1': {'company': '公司 B', 'position': '高级软件工程师', 'start_date': '2016-01-01', 'end_date': '2020-12-31'}}}]\n"
     ]
    }
   ],
   "source": [
    "# 当json里面有一个大列表时，按上述方法会完整保留，这样就会导致片段的长度超长。我们可以指定convert_lists=True来预处理json\n",
    "chunks = json_splitter.split_json(json_data, convert_lists=True)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a717506-165f-431e-b31c-f4823ca42310",
   "metadata": {},
   "source": [
    "# 四、混合分块方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "775bd4e1-e174-45eb-b6aa-fa2eeeb3534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是第一段。这是第一句。这是第二句。', '这是第二段。这是第三句。这是第四句。']\n"
     ]
    }
   ],
   "source": [
    "def hybrid_chunking(text):\n",
    "    paragraph_chunks = paragraph_based_chunking(text)\n",
    "    final_chunks = []\n",
    "    for paragraph in paragraph_chunks:\n",
    "        sentence_chunks = sentence_based_chunking(paragraph)\n",
    "        final_chunks.extend(sentence_chunks)\n",
    "    return final_chunks\n",
    "\n",
    "text = \"这是第一段。这是第一句。这是第二句。\\n\\n这是第二段。这是第三句。这是第四句。\"\n",
    "chunks = hybrid_chunking(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8575c04-f6c1-4b02-96c8-f621347474fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import numpy as np\n",
    "\n",
    "# # 假设我们已经有了一个包含多个文档的列表\n",
    "# documents = [\"文档1的内容\", \"文档2的内容\", ...]\n",
    "\n",
    "# # 使用 CountVectorizer 和 LDA 进行主题建模\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# lda = LDA(n_components=5, random_state=0)\n",
    "# lda.fit(X)\n",
    "\n",
    "# # 获取每个文档的主题分布\n",
    "# topic_distributions = lda.transform(X)\n",
    "\n",
    "# # 简单地按最大概率的主题分配文档\n",
    "# for i, doc in enumerate(documents):\n",
    "#     topic_idx = np.argmax(topic_distributions[i])\n",
    "#     print(f\"Document {i+1} is most likely about topic {topic_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a64790-8622-47a3-b7a9-9a338360543b",
   "metadata": {},
   "source": [
    "请注意，上述代码仅展示了如何确定每个文档的主题，并未实际执行分块操作。要真正实现基于主题的分块，还需要进一步逻辑来将同一主题下的内容合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bfe71-e676-48c6-bcd2-0b039e852e6d",
   "metadata": {},
   "source": [
    "# 基于实体的分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db0fd67-bcfe-4184-a6db-baadbc390a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bill Gates', ' founded ', 'Microsoft Corporation', ' in ', '1975', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 加载 SpaCy 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 示例文本\n",
    "text = \"Bill Gates founded Microsoft Corporation in 1975.\"\n",
    "\n",
    "# 使用 SpaCy 进行命名实体识别\n",
    "doc = nlp(text)\n",
    "\n",
    "# 定义基于实体的分块函数\n",
    "def split_by_entities(text, doc):\n",
    "    entity_spans = [ent for ent in doc.ents]\n",
    "    if not entity_spans:\n",
    "        return [text]  # 如果没有实体，则返回整个文本作为一个分块\n",
    "    \n",
    "    chunks = []\n",
    "    last_end = 0\n",
    "    for span in entity_spans:\n",
    "        start = span.start_char\n",
    "        end = span.end_char\n",
    "        \n",
    "        if start > last_end:\n",
    "            chunks.append(text[last_end:start])  # 添加非实体部分\n",
    "        \n",
    "        chunks.append(text[start:end])  # 添加实体部分\n",
    "        last_end = end\n",
    "    \n",
    "    if last_end < len(text):\n",
    "        chunks.append(text[last_end:])  # 添加剩余部分\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 执行基于实体的分块\n",
    "entity_chunks = split_by_entities(text, doc)\n",
    "print(entity_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a83429-8004-4cdf-873c-2072258498bc",
   "metadata": {},
   "source": [
    "# 基于对话的分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa1dda15-72af-422b-973b-6d01555cfd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: Hello!\n",
      "Bob: Hi Alice, how are you?\n",
      "Alice: I'm good, thanks! What about you?\n",
      "Bob: Doing well, thank you.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 示例对话文本\n",
    "dialogue = \"\"\"\n",
    "Alice: Hello!\n",
    "Bob: Hi Alice, how are you?\n",
    "Alice: I'm good, thanks! What about you?\n",
    "Bob: Doing well, thank you.\n",
    "\"\"\"\n",
    "\n",
    "# 使用正则表达式匹配对话模式\n",
    "pattern = r'([A-Za-z]+):\\s*(.*?)(?=\\n[A-Za-z]+:|$)'\n",
    "\n",
    "# 执行基于对话的分块\n",
    "dialogue_chunks = re.findall(pattern, dialogue, flags=re.DOTALL)\n",
    "for speaker, utterance in dialogue_chunks:\n",
    "    print(f\"{speaker}: {utterance.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349afdd-9a1c-43f7-931c-6a529f8904d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_book",
   "language": "python",
   "name": "rag_book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
